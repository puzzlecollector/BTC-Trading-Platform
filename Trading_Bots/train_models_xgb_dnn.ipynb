{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc3fc8a-5105-4a22-b1b5-3cb71ffdadd8",
   "metadata": {},
   "source": [
    "##### XGBoost Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f634bb04-0c5c-4b2e-a54a-058c766dbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import ccxt \n",
    "from tqdm import tqdm \n",
    "import seaborn as sns \n",
    "from xgboost import XGBClassifier\n",
    "import pandas_ta as ta\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fbd2f7d-6366-4b0e-9e2c-defb93c69839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>datetime</th>\n",
       "      <th>positives</th>\n",
       "      <th>negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>4261.32</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>82.088865</td>\n",
       "      <td>2017-08-17 04:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4333.32</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4333.32</td>\n",
       "      <td>4427.30</td>\n",
       "      <td>63.619882</td>\n",
       "      <td>2017-08-17 08:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4436.06</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4333.42</td>\n",
       "      <td>4352.34</td>\n",
       "      <td>174.562001</td>\n",
       "      <td>2017-08-17 12:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4352.33</td>\n",
       "      <td>4354.84</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4325.23</td>\n",
       "      <td>225.109716</td>\n",
       "      <td>2017-08-17 16:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4307.56</td>\n",
       "      <td>4369.69</td>\n",
       "      <td>4258.56</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>249.769913</td>\n",
       "      <td>2017-08-17 20:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>20735.69</td>\n",
       "      <td>20759.00</td>\n",
       "      <td>20573.13</td>\n",
       "      <td>20722.23</td>\n",
       "      <td>28650.057260</td>\n",
       "      <td>2023-01-15 08:00:00</td>\n",
       "      <td>0.693678</td>\n",
       "      <td>0.306322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11848</th>\n",
       "      <td>20722.23</td>\n",
       "      <td>20994.05</td>\n",
       "      <td>20652.24</td>\n",
       "      <td>20908.28</td>\n",
       "      <td>32504.508940</td>\n",
       "      <td>2023-01-15 12:00:00</td>\n",
       "      <td>0.507753</td>\n",
       "      <td>0.492247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11849</th>\n",
       "      <td>20906.97</td>\n",
       "      <td>21050.74</td>\n",
       "      <td>20665.00</td>\n",
       "      <td>20870.15</td>\n",
       "      <td>37663.667460</td>\n",
       "      <td>2023-01-15 16:00:00</td>\n",
       "      <td>0.537020</td>\n",
       "      <td>0.462980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11850</th>\n",
       "      <td>20870.15</td>\n",
       "      <td>20952.37</td>\n",
       "      <td>20746.39</td>\n",
       "      <td>20871.50</td>\n",
       "      <td>21411.124930</td>\n",
       "      <td>2023-01-15 20:00:00</td>\n",
       "      <td>0.537020</td>\n",
       "      <td>0.462980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11851</th>\n",
       "      <td>20872.99</td>\n",
       "      <td>21439.59</td>\n",
       "      <td>20770.20</td>\n",
       "      <td>21075.68</td>\n",
       "      <td>65696.370430</td>\n",
       "      <td>2023-01-16 00:00:00</td>\n",
       "      <td>0.838868</td>\n",
       "      <td>0.161132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11852 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           open      high       low     close        volume  \\\n",
       "0       4261.48   4349.99   4261.32   4349.99     82.088865   \n",
       "1       4333.32   4485.39   4333.32   4427.30     63.619882   \n",
       "2       4436.06   4485.39   4333.42   4352.34    174.562001   \n",
       "3       4352.33   4354.84   4200.74   4325.23    225.109716   \n",
       "4       4307.56   4369.69   4258.56   4285.08    249.769913   \n",
       "...         ...       ...       ...       ...           ...   \n",
       "11847  20735.69  20759.00  20573.13  20722.23  28650.057260   \n",
       "11848  20722.23  20994.05  20652.24  20908.28  32504.508940   \n",
       "11849  20906.97  21050.74  20665.00  20870.15  37663.667460   \n",
       "11850  20870.15  20952.37  20746.39  20871.50  21411.124930   \n",
       "11851  20872.99  21439.59  20770.20  21075.68  65696.370430   \n",
       "\n",
       "                  datetime  positives  negatives  \n",
       "0      2017-08-17 04:00:00   0.000000   0.000000  \n",
       "1      2017-08-17 08:00:00   0.000000   0.000000  \n",
       "2      2017-08-17 12:00:00   0.000000   0.000000  \n",
       "3      2017-08-17 16:00:00   0.000000   0.000000  \n",
       "4      2017-08-17 20:00:00   0.000000   0.000000  \n",
       "...                    ...        ...        ...  \n",
       "11847  2023-01-15 08:00:00   0.693678   0.306322  \n",
       "11848  2023-01-15 12:00:00   0.507753   0.492247  \n",
       "11849  2023-01-15 16:00:00   0.537020   0.462980  \n",
       "11850  2023-01-15 20:00:00   0.537020   0.462980  \n",
       "11851  2023-01-16 00:00:00   0.838868   0.161132  \n",
       "\n",
       "[11852 rows x 8 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart_df = pd.read_csv(\"chart_with_sentiments.csv\")\n",
    "\n",
    "chart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "551afcfe-cdf4-4138-9508-6481bbe0f208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8895c46d173140eeb51881d0ebc8f2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get datetime features\n",
    "months, days, hours = [], [], [] \n",
    "datetime_values = chart_df[\"datetime\"].values \n",
    "for i in range(len(datetime_values)): \n",
    "    dtobj = pd.to_datetime(datetime_values[i]) \n",
    "    months.append(dtobj.month) \n",
    "    days.append(dtobj.day) \n",
    "    hours.append(dtobj.hour) \n",
    "    \n",
    "chart_df[\"months\"] = months \n",
    "chart_df[\"days\"] = days \n",
    "chart_df[\"hours\"] = hours\n",
    "\n",
    "# construct targets\n",
    "high = chart_df[\"high\"].values \n",
    "low = chart_df[\"low\"].values \n",
    "close = chart_df[\"close\"].values \n",
    "\n",
    "targets = [] \n",
    "\n",
    "for i in range(len(close)-1): \n",
    "    high_vol = (high[i+1] - close[i]) / close[i] \n",
    "    low_vol = (low[i+1] - close[i]) / close[i] \n",
    "    if high_vol >= 0.0075: \n",
    "        targets.append(0) \n",
    "    elif low_vol <= -0.0075: \n",
    "        targets.append(1) \n",
    "    else: \n",
    "        targets.append(2) \n",
    "targets.append(None) \n",
    "\n",
    "chart_df[\"targets\"] = targets \n",
    "\n",
    "# feature engineering \n",
    "chart_df.set_index(pd.DatetimeIndex(chart_df[\"datetime\"]), inplace=True)\n",
    "\n",
    "chart_df[\"bop\"] = chart_df.ta.bop(lookahead=False) \n",
    "chart_df[\"ebsw\"] = chart_df.ta.ebsw(lookahead=False) \n",
    "chart_df[\"cmf\"] = chart_df.ta.cmf(lookahead=False) \n",
    "chart_df[\"vwap\"] = chart_df.ta.vwap(lookahead=False) \n",
    "chart_df[\"rsi/100\"] = chart_df.ta.rsi(lookahead=False) / 100 \n",
    "chart_df[\"high/low\"] = chart_df[\"high\"] / chart_df[\"low\"] \n",
    "chart_df[\"close/open\"] = chart_df[\"close\"] / chart_df[\"open\"] \n",
    "chart_df[\"high/open\"] = chart_df[\"high\"] / chart_df[\"open\"] \n",
    "chart_df[\"low/open\"] = chart_df[\"low\"] / chart_df[\"open\"] \n",
    "chart_df[\"hwma\"] = chart_df.ta.hwma(lookahead=False) \n",
    "chart_df[\"linreg\"] = chart_df.ta.linreg(lookahead=False) \n",
    "chart_df[\"hwma/close\"] = chart_df[\"hwma\"] / chart_df[\"close\"] \n",
    "chart_df[\"linreg/close\"] = chart_df[\"linreg\"] / chart_df[\"close\"] \n",
    "\n",
    "for l in tqdm(range(1, 12), position = 0, leave=True): \n",
    "    for col in [\"open\", \"high\", \"low\", \"close\", \"volume\", \"vwap\"]: \n",
    "        val = chart_df[col].values \n",
    "        val_ret = [None for _  in range(l)] \n",
    "        for i in range(l, len(val)):\n",
    "            if val[i-l] == 0: \n",
    "                ret = 1 \n",
    "            else: \n",
    "                ret = val[i] / val[i-l]  \n",
    "            val_ret.append(ret) \n",
    "        chart_df[\"{}_change_{}\".format(col, l)] = val_ret \n",
    "        \n",
    "chart_df.drop(columns={\"open\", \"high\", \"low\", \"close\", \"volume\", \"vwap\", \"hwma\", \"linreg\", \"datetime\"}, inplace=True) \n",
    "chart_df.dropna(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8dc477c9-ec5e-4cff-9d43-3fad332f4fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positives</th>\n",
       "      <th>negatives</th>\n",
       "      <th>months</th>\n",
       "      <th>days</th>\n",
       "      <th>hours</th>\n",
       "      <th>targets</th>\n",
       "      <th>bop</th>\n",
       "      <th>ebsw</th>\n",
       "      <th>cmf</th>\n",
       "      <th>rsi/100</th>\n",
       "      <th>...</th>\n",
       "      <th>low_change_10</th>\n",
       "      <th>close_change_10</th>\n",
       "      <th>volume_change_10</th>\n",
       "      <th>vwap_change_10</th>\n",
       "      <th>open_change_11</th>\n",
       "      <th>high_change_11</th>\n",
       "      <th>low_change_11</th>\n",
       "      <th>close_change_11</th>\n",
       "      <th>volume_change_11</th>\n",
       "      <th>vwap_change_11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-15 04:00:00</th>\n",
       "      <td>0.604706</td>\n",
       "      <td>0.395294</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.135289</td>\n",
       "      <td>0.675729</td>\n",
       "      <td>0.267520</td>\n",
       "      <td>0.788829</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098629</td>\n",
       "      <td>1.076553</td>\n",
       "      <td>0.238653</td>\n",
       "      <td>1.093601</td>\n",
       "      <td>1.100616</td>\n",
       "      <td>1.090876</td>\n",
       "      <td>1.098717</td>\n",
       "      <td>1.095912</td>\n",
       "      <td>0.378044</td>\n",
       "      <td>1.099542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-15 08:00:00</th>\n",
       "      <td>0.693678</td>\n",
       "      <td>0.306322</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.072416</td>\n",
       "      <td>0.096958</td>\n",
       "      <td>0.271411</td>\n",
       "      <td>0.784283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.079379</td>\n",
       "      <td>1.070252</td>\n",
       "      <td>0.376537</td>\n",
       "      <td>1.087931</td>\n",
       "      <td>1.095912</td>\n",
       "      <td>1.074988</td>\n",
       "      <td>1.093645</td>\n",
       "      <td>1.075854</td>\n",
       "      <td>0.337209</td>\n",
       "      <td>1.092503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-15 12:00:00</th>\n",
       "      <td>0.507753</td>\n",
       "      <td>0.492247</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.544308</td>\n",
       "      <td>-0.693251</td>\n",
       "      <td>0.293178</td>\n",
       "      <td>0.801327</td>\n",
       "      <td>...</td>\n",
       "      <td>1.069191</td>\n",
       "      <td>1.049085</td>\n",
       "      <td>0.413732</td>\n",
       "      <td>1.081280</td>\n",
       "      <td>1.075854</td>\n",
       "      <td>1.081818</td>\n",
       "      <td>1.083530</td>\n",
       "      <td>1.079861</td>\n",
       "      <td>0.427194</td>\n",
       "      <td>1.089706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-15 16:00:00</th>\n",
       "      <td>0.537020</td>\n",
       "      <td>0.462980</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.095453</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.274325</td>\n",
       "      <td>0.787593</td>\n",
       "      <td>...</td>\n",
       "      <td>1.039066</td>\n",
       "      <td>0.995370</td>\n",
       "      <td>0.283385</td>\n",
       "      <td>1.003903</td>\n",
       "      <td>1.079714</td>\n",
       "      <td>1.052537</td>\n",
       "      <td>1.069851</td>\n",
       "      <td>1.047172</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>1.082540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-15 20:00:00</th>\n",
       "      <td>0.537020</td>\n",
       "      <td>0.462980</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006554</td>\n",
       "      <td>0.860109</td>\n",
       "      <td>0.329369</td>\n",
       "      <td>0.787731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998509</td>\n",
       "      <td>0.998763</td>\n",
       "      <td>0.479250</td>\n",
       "      <td>1.001902</td>\n",
       "      <td>1.047172</td>\n",
       "      <td>0.985623</td>\n",
       "      <td>1.043159</td>\n",
       "      <td>0.995434</td>\n",
       "      <td>0.161099</td>\n",
       "      <td>1.004317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     positives  negatives  months  days  hours  targets  \\\n",
       "datetime                                                                  \n",
       "2023-01-15 04:00:00   0.604706   0.395294       1    15      4      1.0   \n",
       "2023-01-15 08:00:00   0.693678   0.306322       1    15      8      0.0   \n",
       "2023-01-15 12:00:00   0.507753   0.492247       1    15     12      1.0   \n",
       "2023-01-15 16:00:00   0.537020   0.462980       1    15     16      2.0   \n",
       "2023-01-15 20:00:00   0.537020   0.462980       1    15     20      0.0   \n",
       "\n",
       "                          bop      ebsw       cmf   rsi/100  ...  \\\n",
       "datetime                                                     ...   \n",
       "2023-01-15 04:00:00  0.135289  0.675729  0.267520  0.788829  ...   \n",
       "2023-01-15 08:00:00 -0.072416  0.096958  0.271411  0.784283  ...   \n",
       "2023-01-15 12:00:00  0.544308 -0.693251  0.293178  0.801327  ...   \n",
       "2023-01-15 16:00:00 -0.095453  0.158438  0.274325  0.787593  ...   \n",
       "2023-01-15 20:00:00  0.006554  0.860109  0.329369  0.787731  ...   \n",
       "\n",
       "                     low_change_10  close_change_10  volume_change_10  \\\n",
       "datetime                                                                \n",
       "2023-01-15 04:00:00       1.098629         1.076553          0.238653   \n",
       "2023-01-15 08:00:00       1.079379         1.070252          0.376537   \n",
       "2023-01-15 12:00:00       1.069191         1.049085          0.413732   \n",
       "2023-01-15 16:00:00       1.039066         0.995370          0.283385   \n",
       "2023-01-15 20:00:00       0.998509         0.998763          0.479250   \n",
       "\n",
       "                     vwap_change_10  open_change_11  high_change_11  \\\n",
       "datetime                                                              \n",
       "2023-01-15 04:00:00        1.093601        1.100616        1.090876   \n",
       "2023-01-15 08:00:00        1.087931        1.095912        1.074988   \n",
       "2023-01-15 12:00:00        1.081280        1.075854        1.081818   \n",
       "2023-01-15 16:00:00        1.003903        1.079714        1.052537   \n",
       "2023-01-15 20:00:00        1.001902        1.047172        0.985623   \n",
       "\n",
       "                     low_change_11  close_change_11  volume_change_11  \\\n",
       "datetime                                                                \n",
       "2023-01-15 04:00:00       1.098717         1.095912          0.378044   \n",
       "2023-01-15 08:00:00       1.093645         1.075854          0.337209   \n",
       "2023-01-15 12:00:00       1.083530         1.079861          0.427194   \n",
       "2023-01-15 16:00:00       1.069851         1.047172          0.479400   \n",
       "2023-01-15 20:00:00       1.043159         0.995434          0.161099   \n",
       "\n",
       "                     vwap_change_11  \n",
       "datetime                             \n",
       "2023-01-15 04:00:00        1.099542  \n",
       "2023-01-15 08:00:00        1.092503  \n",
       "2023-01-15 12:00:00        1.089706  \n",
       "2023-01-15 16:00:00        1.082540  \n",
       "2023-01-15 20:00:00        1.004317  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0a84b27f-204b-4f05-a96c-bfdc7aeef6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='targets', ylabel='count'>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(chart_df, x = \"targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aa805de8-8eea-4181-9149-82a626fa69bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9449, 82), (1181, 82), (1182, 82))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.8 * chart_df.shape[0]) \n",
    "val_size = int(0.1 * chart_df.shape[0]) \n",
    "\n",
    "train_df = chart_df.iloc[:train_size] \n",
    "val_df = chart_df.iloc[train_size:train_size+val_size] \n",
    "test_df = chart_df.iloc[train_size+val_size:] \n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "076473ab-54dc-40cf-94be-fc8905ba12a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9449, 81), (9449,), (1181, 81), (1181,), (1182, 81), (1182,))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_columns = [] \n",
    "for col in chart_df.columns: \n",
    "    if col != \"targets\":\n",
    "        train_columns.append(col) \n",
    "        \n",
    "X_train = train_df[train_columns] \n",
    "Y_train = train_df[\"targets\"] \n",
    "\n",
    "X_val = val_df[train_columns] \n",
    "Y_val = val_df[\"targets\"] \n",
    "\n",
    "X_test = test_df[train_columns] \n",
    "Y_test = test_df[\"targets\"] \n",
    "\n",
    "X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c42e1f6f-7d22-4d0a-ac9b-8505883bfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight = \"balanced\",\n",
    "                                     classes = np.unique(Y_train),\n",
    "                                     y = Y_train) \n",
    "\n",
    "d = {0:class_weights[0], 1:class_weights[1], 2:class_weights[2]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c4e78bd-e367-4630-8125-f759f2819bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:25:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"metric\", \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[08:25:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.05196\n",
      "[20]\tvalidation_0-mlogloss:0.96561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalidation_0-mlogloss:0.97146\n",
      "[60]\tvalidation_0-mlogloss:0.97634\n",
      "[80]\tvalidation_0-mlogloss:0.98124\n",
      "[99]\tvalidation_0-mlogloss:0.98610\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "              class_weight={0: 0.6677266624266837, 1: 1.13256622318111,\n",
       "                            2: 1.6143857850674868},\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              enable_categorical=False, gamma=0, gpu_id=0, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=3, metric=&#x27;logloss&#x27;,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=&#x27;()&#x27;,\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, silent=False,\n",
       "              subsample=1, tree_method=&#x27;gpu_hist&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "              class_weight={0: 0.6677266624266837, 1: 1.13256622318111,\n",
       "                            2: 1.6143857850674868},\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              enable_categorical=False, gamma=0, gpu_id=0, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=3, metric=&#x27;logloss&#x27;,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=&#x27;()&#x27;,\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, silent=False,\n",
       "              subsample=1, tree_method=&#x27;gpu_hist&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "              class_weight={0: 0.6677266624266837, 1: 1.13256622318111,\n",
       "                            2: 1.6143857850674868},\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              enable_categorical=False, gamma=0, gpu_id=0, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=3, metric='logloss',\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, silent=False,\n",
       "              subsample=1, tree_method='gpu_hist', ...)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(silent=False, \n",
    "                    n_estimators=100,\n",
    "                    class_weight=d, \n",
    "                    metric=\"logloss\", \n",
    "                    tree_method=\"gpu_hist\",\n",
    "                    max_depth=3)\n",
    "\n",
    "clf.fit(X_train, \n",
    "        Y_train, \n",
    "        eval_set=[(X_val, Y_val)],\n",
    "        verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce45dbf-e1cd-401f-a4cd-3a299938494b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.90693739424704, 0.5285295506919703)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = clf.predict(X_test) \n",
    "\n",
    "gt = Y_test.values \n",
    "\n",
    "cnt = 0 \n",
    "\n",
    "for i in range(len(Y_pred)): \n",
    "    if Y_pred[i] == gt[i]: \n",
    "        cnt += 1 \n",
    "        \n",
    "accuracy = cnt / len(Y_pred) * 100 \n",
    "f1 = f1_score(gt, Y_pred, average=\"weighted\") \n",
    "\n",
    "accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1722ca8c-7f56-419b-b06b-34784216b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save_model(\"CBITS_XGB\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0000353-4d1b-4ff8-a5de-b8c5a6f2890b",
   "metadata": {},
   "source": [
    "##### TabNet Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23a6a0e4-b82e-4b07-8601-2199943650b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.4614  | val_0_logloss: 1.16878 |  0:00:00s\n",
      "epoch 20 | loss: 0.93064 | val_0_logloss: 1.06675 |  0:00:10s\n",
      "epoch 40 | loss: 0.92094 | val_0_logloss: 1.05152 |  0:00:20s\n",
      "epoch 60 | loss: 0.90968 | val_0_logloss: 1.04067 |  0:00:30s\n",
      "epoch 80 | loss: 0.90393 | val_0_logloss: 1.01041 |  0:00:39s\n",
      "epoch 100| loss: 0.90125 | val_0_logloss: 1.04789 |  0:00:49s\n",
      "epoch 120| loss: 0.8927  | val_0_logloss: 1.00897 |  0:00:59s\n",
      "epoch 140| loss: 0.88503 | val_0_logloss: 1.04062 |  0:01:09s\n",
      "epoch 160| loss: 0.88589 | val_0_logloss: 1.03434 |  0:01:19s\n",
      "epoch 180| loss: 0.88734 | val_0_logloss: 1.03069 |  0:01:29s\n",
      "epoch 200| loss: 0.88242 | val_0_logloss: 1.04154 |  0:01:38s\n",
      "epoch 220| loss: 0.86302 | val_0_logloss: 1.03255 |  0:01:48s\n",
      "epoch 240| loss: 0.86634 | val_0_logloss: 1.02388 |  0:01:57s\n",
      "epoch 260| loss: 0.85966 | val_0_logloss: 1.04036 |  0:02:08s\n",
      "epoch 280| loss: 0.8502  | val_0_logloss: 1.05305 |  0:02:18s\n",
      "Stop training because you reached max_epochs = 300 with best_epoch = 249 and best_val_0_logloss = 1.00411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier \n",
    "\n",
    "tab_clf = TabNetClassifier(n_steps=4, verbose=20) \n",
    "tab_clf.fit(X_train.values, \n",
    "            Y_train.values, \n",
    "            eval_set=[(X_val.values, Y_val.values)], \n",
    "            eval_metric=[\"logloss\"], \n",
    "            max_epochs = 300, \n",
    "            patience = 300, # no early stopping, \n",
    "            weights = d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f902dea-a8d9-4139-a0e4-ab7f42f1caa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51.18443316412859, 0.4947985822514664)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = tab_clf.predict(X_test.values) \n",
    "\n",
    "gt = Y_test.values \n",
    "\n",
    "cnt = 0 \n",
    "\n",
    "for i in range(len(Y_pred)): \n",
    "    if Y_pred[i] == gt[i]: \n",
    "        cnt += 1 \n",
    "        \n",
    "accuracy = cnt / len(Y_pred) * 100 \n",
    "f1 = f1_score(gt, Y_pred, average=\"weighted\") \n",
    "\n",
    "accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63d20b94-c3da-4555-a25e-786e5581074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_prob = tab_clf.predict_proba(X_test.values) \n",
    "xgb_prob = clf.predict_proba(X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6b06e22-7f2e-4ed4-9d13-4c39559524bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_prob = (tab_prob + xgb_prob) / 2.0 \n",
    "avg_class = np.argmax(avg_prob, axis=1) \n",
    "avg_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db7196c1-f134-40f5-881e-4e6f6228d12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5423011844331641, 0.5237642229149104)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0 \n",
    "for i in range(len(avg_class)): \n",
    "    if gt[i] == avg_class[i]:\n",
    "        cnt += 1 \n",
    "        \n",
    "accuracy = cnt / len(avg_class) \n",
    "f1 = f1_score(gt, avg_class, average=\"weighted\") \n",
    "\n",
    "accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94edc1d2-0129-4466-b477-a87b9d733190",
   "metadata": {},
   "source": [
    "##### Catboost Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e06563-df5a-4e55-a56f-ac28c2e754fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6675e684-c944-494a-ac39-cc3b0181f0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1902c90-7647-4e09-9da2-fa36404ea26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3dc9c40-b709-4549-88c8-2c64a9dd0739",
   "metadata": {},
   "source": [
    "##### DNN Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5ee1473a-6654-4e17-a7f6-c68b4606cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, f1_score \n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "from tqdm.auto import tqdm \n",
    "from sklearn import preprocessing\n",
    "import time \n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from transformers import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "46bab5ba-151c-4375-a631-9fa7c224cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSampleDropout(nn.Module):\n",
    "    def __init__(self, max_dropout_rate, num_samples, classifier): \n",
    "        super(MultiSampleDropout, self).__init__() \n",
    "        self.dropout = nn.Dropout\n",
    "        self.classifier = classifier \n",
    "        self.max_dropout_rate = max_dropout_rate \n",
    "        self.num_samples = num_samples\n",
    "    def forward(self, out): \n",
    "        return torch.mean(torch.stack([self.classifier(self.dropout(p=rate)(out)) for _, rate in enumerate(np.linspace(0, self.max_dropout_rate, self.num_samples))], dim=0), dim=0)\n",
    "\n",
    "class AttentivePooling(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentivePooling, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "    def forward(self, x):\n",
    "        softmax = F.softmax\n",
    "        att_w = softmax(self.W(x).squeeze(-1)).unsqueeze(-1)\n",
    "        x = torch.sum(x * att_w, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DNN(nn.Module): \n",
    "    def __init__(self, num_classes, num_features): \n",
    "        super(DNN, self).__init__() \n",
    "        self.num_classes = num_classes \n",
    "        self.num_features = num_features\n",
    "        self.batchnorm = nn.BatchNorm1d(self.num_features) \n",
    "        self.fc = nn.Linear(self.num_features, 128) \n",
    "        self.fc2 = nn.Linear(128, 64) \n",
    "        self.fc3 = nn.Linear(64, self.num_classes) \n",
    "        self.multi_dropout = MultiSampleDropout(0.2, 4, self.fc3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm(x) \n",
    "        x = self.fc(x) \n",
    "        x = self.fc2(x) \n",
    "        x = self.multi_dropout(x) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9d467d81-8b4b-48d0-bdff-44d7300abb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pt = torch.tensor(X_train.values, dtype=torch.float32) \n",
    "Y_train_pt = torch.tensor(Y_train.values, dtype=int) \n",
    "\n",
    "X_val_pt = torch.tensor(X_val.values, dtype=torch.float32) \n",
    "Y_val_pt = torch.tensor(Y_val.values, dtype=int) \n",
    "\n",
    "X_test_pt = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_pt = torch.tensor(Y_test.values, dtype=int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8dee2b9f-e9ab-4249-a8e5-2912f59b61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "train_data = TensorDataset(X_train_pt, Y_train_pt) \n",
    "train_sampler = RandomSampler(train_data) \n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) \n",
    "\n",
    "val_data = TensorDataset(X_val_pt, Y_val_pt) \n",
    "val_sampler = SequentialSampler(val_data) \n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size) \n",
    "\n",
    "test_data = TensorDataset(X_test_pt, Y_test_pt) \n",
    "test_sampler = SequentialSampler(test_data) \n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5750ce3c-ce79-416e-b17a-114766e024e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_268/3892336605.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_weights = torch.tensor(class_weights).float().to(device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a312bf809b8544e9b7b70abf01e493df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5294d7cfa8144f85b4cd1e4a12f12b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1060b973b364764b65edbe818909e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9627244118097666 | avg train accuracy : 0.4868735923423424 | avg val loss : 1.0191626291017275 | avg val accuracy : 0.41658900279589933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ceb6b40674487296b94aae83aee1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ad1a03593a468588239d3badd9fd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9302505701780319 | avg train accuracy : 0.5087274774774775 | avg val loss : 0.9939614067206511 | avg val accuracy : 0.45080964585274924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe863b2f16c4cfc9a823de521fcfcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb6913446a14d04a6036585c5500ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9193206888598364 | avg train accuracy : 0.5149798235735735 | avg val loss : 0.993248066386661 | avg val accuracy : 0.46481826654240443\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94815b0d9fc446e8ff1cffd8645a02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dd8c41bd33433a9897ab93a7f10aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9138120728972796 | avg train accuracy : 0.5194139451951951 | avg val loss : 0.9771459344271067 | avg val accuracy : 0.44979030754892824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f408c9fc4264530ac1e35c65df00b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91c9a9e607b43fe87cc8ad0eae149db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9216425265814807 | avg train accuracy : 0.5146396396396397 | avg val loss : 0.9803588535334613 | avg val accuracy : 0.4453925908667288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1174f9eaf39544c18149c11d35dd7797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f705bf4b79647da99b5d5e5261cacfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9127561705740722 | avg train accuracy : 0.5196485548048049 | avg val loss : 0.9956894500835521 | avg val accuracy : 0.4437034016775396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bd2e497d6c43d29e81b8ab41d0a5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1472b525d484c1ebec942b1d9b99647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9160235834282797 | avg train accuracy : 0.5207160285285286 | avg val loss : 0.9902594733882595 | avg val accuracy : 0.44792637465051255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5515fd202b324224a941b107df2f12aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516cec59912248309fc232748018a02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9098006792970605 | avg train accuracy : 0.518041478978979 | avg val loss : 0.9894577715847943 | avg val accuracy : 0.453925908667288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20b2a61382f4af5a9d13de8946641fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc00e9fd30b4dd98083b636ce8c8702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9083544954254821 | avg train accuracy : 0.5227688626126127 | avg val loss : 0.9927204589586001 | avg val accuracy : 0.43703401677539605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c5f89a77284c0b8cab4f704c449756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e061a16068864fba9ff366cff2415f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.9056839524088679 | avg train accuracy : 0.5224052177177178 | avg val loss : 0.9861056724110165 | avg val accuracy : 0.4328110438024231\n"
     ]
    }
   ],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() \n",
    "    labels_flat = labels.flatten() \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat) \n",
    "\n",
    "device = torch.device(\"cuda\") \n",
    "best_val_loss = 99999999\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "class_weights = torch.tensor(class_weights).float().to(device) \n",
    "loss_func = nn.CrossEntropyLoss(weight=class_weights) \n",
    "model = DNN(num_classes=3, num_features=X_train.shape[1])\n",
    "model.to(device) \n",
    "optimizer = AdamW(model.parameters(), lr=1e-3) \n",
    "epochs = 10 \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps) \n",
    "\n",
    "model.zero_grad() \n",
    "for epoch_i in tqdm(range(epochs), desc=\"Epochs\", position=0, leave=True, total=epochs): \n",
    "    train_loss, train_accuracy = 0, 0 \n",
    "    model.train() \n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch: \n",
    "        for step, batch in enumerate(tepoch): \n",
    "            batch = tuple(t.to(device) for t in batch) \n",
    "            b_x, b_labels = batch \n",
    "            output = model(b_x) \n",
    "            loss = loss_func(output, b_labels) \n",
    "            logits_cpu, labels_cpu = output.detach().cpu().numpy(), b_labels.detach().cpu().numpy() \n",
    "            train_accuracy += flat_accuracy(logits_cpu, labels_cpu) \n",
    "            train_loss += loss.item() \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "            optimizer.step() \n",
    "            scheduler.step() \n",
    "            model.zero_grad() \n",
    "            tepoch.set_postfix(loss=train_loss/(step+1), accuracy=100.0 * train_accuracy / (step+1)) \n",
    "            time.sleep(0.1) \n",
    "        avg_train_loss = train_loss / len(train_dataloader) \n",
    "        avg_train_accuracy = train_accuracy / len(train_dataloader) \n",
    "        train_losses.append(avg_train_loss) \n",
    "        train_accuracies.append(avg_train_accuracy) \n",
    "    val_loss, val_accuracy = 0, 0 \n",
    "    model.eval() \n",
    "    for step, batch in tqdm(enumerate(val_dataloader), position=0, leave=True, total=len(val_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        b_x, b_labels = batch \n",
    "        with torch.no_grad(): \n",
    "            output = model(b_x) \n",
    "        loss = loss_func(output, b_labels) \n",
    "        val_loss += loss.item() \n",
    "        logits_cpu, labels_cpu = output.detach().cpu().numpy(), b_labels.detach().cpu().numpy() \n",
    "        val_accuracy += flat_accuracy(logits_cpu, labels_cpu) \n",
    "    avg_val_loss = val_loss / len(val_dataloader) \n",
    "    avg_val_accuracy = val_accuracy / len(val_dataloader) \n",
    "    val_losses.append(avg_val_loss) \n",
    "    val_accuracies.append(avg_val_accuracy) \n",
    "    print(f\"avg train loss : {avg_train_loss} | avg train accuracy : {avg_train_accuracy} | avg val loss : {avg_val_loss} | avg val accuracy : {avg_val_accuracy}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss \n",
    "        torch.save(model.state_dict(), \"DNN_chkpt.pt\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "17885875-03ce-47ae-a301-f7d0babdc80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea80d08d90f4603b10d5e120dbc81f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_268/2347588174.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = nn.Softmax()(output)\n"
     ]
    }
   ],
   "source": [
    "test_model = DNN(num_classes=3, num_features=X_train.shape[1])\n",
    "checkpoint = torch.load(\"DNN_chkpt.pt\") \n",
    "test_model.load_state_dict(checkpoint) \n",
    "test_model.eval() \n",
    "test_model.to(device) \n",
    "\n",
    "predictions = [] \n",
    "logits = [] \n",
    "\n",
    "for batch in tqdm(test_dataloader): \n",
    "    batch = (t.to(device) for t in batch) \n",
    "    b_x, b_labels = batch \n",
    "    with torch.no_grad(): \n",
    "        output = test_model(b_x) \n",
    "        output = nn.Softmax()(output) \n",
    "        classes = torch.argmax(output, dim=1) \n",
    "    for i in range(len(classes)): \n",
    "        predictions.append(classes[i].detach().cpu().item()) \n",
    "        logits.append(output[i].detach().cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fb588ecd-123e-4123-8d5c-0a8df0432454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5423011844331641, 0.4862521666714149)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = Y_test.values\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for i in range(len(gt)): \n",
    "    if gt[i] == predictions[i]: \n",
    "        cnt += 1 \n",
    "        \n",
    "accuray = cnt / len(predictions) \n",
    "f1 = f1_score(gt, predictions, average=\"weighted\") \n",
    "\n",
    "accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9fca1-8d63-4324-9e74-a4eb09504bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b2519a82-be17-4a12-b229-935673c7b641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13640852, 0.23191854, 0.6316729 ],\n",
       "       [0.31705412, 0.43544877, 0.24749713],\n",
       "       [0.23393981, 0.34032583, 0.42573437],\n",
       "       ...,\n",
       "       [0.31486586, 0.36105296, 0.3240812 ],\n",
       "       [0.33122894, 0.4065627 , 0.26220834],\n",
       "       [0.3059556 , 0.40535522, 0.2886892 ]], dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = np.array(logits) \n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "727961c4-6704-4483-8d68-1aefdd229165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26562855, 0.14613858, 0.5882328 ],\n",
       "       [0.35263222, 0.37139103, 0.27597675],\n",
       "       [0.42233625, 0.2489828 , 0.32868096],\n",
       "       ...,\n",
       "       [0.33861297, 0.51957715, 0.1418099 ],\n",
       "       [0.45136967, 0.46095467, 0.08767569],\n",
       "       [0.5155421 , 0.3371213 , 0.14733662]], dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d7c11d03-ebb4-4c26-8b84-08d4b8f0ed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12363117, 0.16417351, 0.71219534],\n",
       "       [0.33966345, 0.3294205 , 0.33091602],\n",
       "       [0.23677988, 0.35692218, 0.4062979 ],\n",
       "       ...,\n",
       "       [0.40086564, 0.4980307 , 0.10110357],\n",
       "       [0.42081636, 0.46666792, 0.11251572],\n",
       "       [0.44093192, 0.42606866, 0.13299942]], dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "351aeb61-f36c-46bb-9104-1ea5c230f574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55.0761421319797, 0.534794764625418)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_prob = (xgb_prob + logits) / 2 \n",
    "avg_class = np.argmax(avg_prob, axis=1)\n",
    "\n",
    "cnt = 0 \n",
    "\n",
    "for i in range(len(gt)): \n",
    "    if gt[i] == avg_class[i]: \n",
    "        cnt += 1 \n",
    "        \n",
    "        \n",
    "cnt / len(avg_prob) * 100, f1_score(gt, avg_class, average=\"weighted\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f20000-c455-4b3f-aa26-fd9788824828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd573746-306f-4a16-bb95-92618572290a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57881c0-a4fa-4d6d-9897-90575a23fcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ace7a2-e27d-4ac5-a8c1-19b90b8f24c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e61f8c-1fbc-460f-b462-e6d89025c86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
